#!/usr/bin/env python3
"""ä¸Šä¼ æœªå‹ç¼©æ¨¡å‹åˆ° S3ï¼ˆè·³è¿‡æ‰“åŒ…ï¼ŒåŠ é€Ÿéƒ¨ç½²ï¼‰"""
import os
import json
import boto3
import warnings
from concurrent.futures import ThreadPoolExecutor

warnings.filterwarnings('ignore', category=DeprecationWarning)

def load_config():
    config_file = 'deploy_vars.json'
    if os.path.exists(config_file):
        with open(config_file) as f:
            return json.load(f)
    return {'AWS_REGION': 'us-west-2', 'MODEL_ID': 'unknown-model'}

config = load_config()
REGION = config['AWS_REGION']
MODEL_ID = config['MODEL_ID']
LOCAL_MODEL_DIR = os.environ.get('LOCAL_MODEL_DIR', 'model')

# ä» MODEL_ID ç”Ÿæˆå”¯ä¸€çš„ S3 å‰ç¼€
model_name = MODEL_ID.split('/')[-1].lower().replace('_', '-')
S3_PREFIX = f"models/{model_name}"

uploaded_count = 0
total_files = 0

def upload_file(args):
    global uploaded_count
    s3_client, bucket, local_path, s3_key = args
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°ä¸€è‡´
        local_size = os.path.getsize(local_path)
        try:
            response = s3_client.head_object(Bucket=bucket, Key=s3_key)
            s3_size = response['ContentLength']
            if s3_size == local_size:
                uploaded_count += 1
                if uploaded_count % 10 == 0:
                    print(f"  å·²éªŒè¯: {uploaded_count}/{total_files}")
                return s3_key
        except:
            pass  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸Šä¼ 
        
        s3_client.upload_file(local_path, bucket, s3_key)
        uploaded_count += 1
        if uploaded_count % 10 == 0:
            print(f"  å·²ä¸Šä¼ : {uploaded_count}/{total_files}")
        return s3_key
    except Exception as e:
        print(f"\nâŒ ä¸Šä¼ å¤±è´¥: {local_path}\né”™è¯¯: {e}")
        raise

def main():
    if not os.path.exists(LOCAL_MODEL_DIR) or not os.listdir(LOCAL_MODEL_DIR):
        print("âŒ è¯·å…ˆè¿è¡Œ: python 1_download_model.py")
        return
    
    sts = boto3.client('sts', region_name=REGION)
    account_id = sts.get_caller_identity()['Account']
    bucket = f"sagemaker-{REGION}-{account_id}"
    s3_client = boto3.client('s3', region_name=REGION)
    
    # åˆ›å»º bucketï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    try:
        s3_client.head_bucket(Bucket=bucket)
        print(f"âœ“ S3 bucket å·²å­˜åœ¨: {bucket}")
    except:
        print(f"åˆ›å»º S3 bucket: {bucket}")
        try:
            if REGION == 'us-east-1':
                s3_client.create_bucket(Bucket=bucket)
            else:
                s3_client.create_bucket(
                    Bucket=bucket,
                    CreateBucketConfiguration={'LocationConstraint': REGION}
                )
            print(f"âœ“ Bucket åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆ›å»º bucket å¤±è´¥: {e}")
            raise
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    files = []
    for root, _, filenames in os.walk(LOCAL_MODEL_DIR):
        for f in filenames:
            local_path = os.path.join(root, f)
            rel_path = os.path.relpath(local_path, LOCAL_MODEL_DIR)
            s3_key = f"{S3_PREFIX}/{rel_path}"
            files.append((s3_client, bucket, local_path, s3_key))
    
    global total_files
    total_files = len(files)
    print(f"ğŸ“¦ æ¨¡å‹: {MODEL_ID}")
    print(f"â³ ä¸Šä¼  {total_files} ä¸ªæ–‡ä»¶åˆ° s3://{bucket}/{S3_PREFIX}/")
    
    # å¹¶è¡Œä¸Šä¼ 
    try:
        with ThreadPoolExecutor(max_workers=10) as executor:
            list(executor.map(upload_file, files))
    except Exception as e:
        print(f"\nâŒ ä¸Šä¼ ä¸­æ–­")
        raise
    
    model_data_url = f"s3://{bucket}/{S3_PREFIX}/"
    
    config = {
        "model_data_url": model_data_url,
        "compression": "None",
        "region": REGION
    }
    with open("config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    print(f"\nâœ… å®Œæˆ: {model_data_url}")

if __name__ == "__main__":
    main()
